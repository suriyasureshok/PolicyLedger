"""
ğŸš€ CYBER DEFENSE DEMO â€” 6 Agents Training Decision Policies ğŸ¯

Demonstrates PolicyLedger with simulated cyber defense environment.
This is a DECISION-LEVEL simulation, not a real cybersecurity system.

Shows complete workflow:
- Decentralized agent training (cyber defense policies)
- Deterministic verification replay
- Tamper-evident ledger storage
- Policy marketplace ranking
- Zero-training policy reuse
"""

import time
from datetime import datetime

print("=" * 80)
print("ğŸš€ CYBER DEFENSE POLICY MARKETPLACE DEMO ğŸ¯")
print("=" * 80)
print()
print("âš¡ Training: 500 episodes per agent for robust policy learning")
print()
print("This demonstrates decentralized learning for simulated cyber defense:")
print("  â€¢ 6 independent agents train defense policies with different seeds")
print("  â€¢ Each agent learns decision-level cyber defense strategies")
print("  â€¢ No coordination between agents (decentralized)")
print("  â€¢ Policies are verified through deterministic replay")
print()
print("=" * 80)
print()

overall_start = time.time()

# -----------------------------------------------------------------------------
# STEP 1: Train All Agents Sequentially
# -----------------------------------------------------------------------------
print("ğŸ“š STEP 1: Train 6 Independent Cyber Defense Agents")
print("-" * 80)

from src.agent.runner import run_agent

AGENT_CONFIGS = [
    ("agent_alpha", 42, 500),
    ("agent_beta", 99, 500),
    ("agent_gamma", 123, 500),
    ("agent_delta", 256, 500),
    ("agent_epsilon", 777, 500),
    ("agent_zeta", 1024, 500),
]

claims = []
total_training_time = 0

for agent_id, seed, episodes in AGENT_CONFIGS:
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] ğŸ¤– {agent_id} â†’ Training defense policy (seed={seed}, episodes={episodes})")
    
    start_time = time.time()
    claim = run_agent(agent_id, seed, episodes)
    elapsed = time.time() - start_time
    total_training_time += elapsed
    
    claims.append(claim)
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] âœ… {agent_id} â†’ Done in {elapsed:.1f}s | Defense Score: {claim.claimed_reward:.3f}")

training_time = time.time() - overall_start

print(f"\n{'=' * 80}")
print(f"âœ… All {len(claims)} agents trained in {training_time:.1f}s")
print("=" * 80)
print()

# -----------------------------------------------------------------------------
# STEP 2: Display Results
# -----------------------------------------------------------------------------
print("ğŸ“Š STEP 2: Defense Policy Training Results")
print("-" * 80)

claims_sorted = sorted(claims, key=lambda c: c.claimed_reward, reverse=True)
for i, claim in enumerate(claims_sorted, 1):
    bar = "â–ˆ" * max(0, int(claim.claimed_reward * 2))
    print(f"  {i}. {claim.agent_id:15s} â†’ {claim.claimed_reward:7.3f} {bar}")
print()

# -----------------------------------------------------------------------------
# STEP 3: Verify All Claims
# -----------------------------------------------------------------------------
print("=" * 80)
print("ğŸ” STEP 3: Verify Defense Policies via Deterministic Replay")
print("-" * 80)
print("Verifier replays each policy in simulation to confirm claimed scores...")
print()

from src.verifier.verifier import PolicyVerifier

verifier = PolicyVerifier()
verified_results = []

verify_start = time.time()
for claim in claims:
    result = verifier.verify(claim)
    verified_results.append(result)
    
    status_icon = "âœ…" if result.status.value == "VALID" else "âŒ"
    print(f"{status_icon} {claim.agent_id:15s} â†’ Verified Defense Score: {result.verified_reward:.3f}")

verify_time = time.time() - verify_start
valid_count = sum(1 for r in verified_results if r.status.value == "VALID")
print(f"\nâœ… Verified {valid_count}/{len(claims)} policies in {verify_time:.1f}s")
print()

# -----------------------------------------------------------------------------
# STEP 4: Record in Ledger
# -----------------------------------------------------------------------------
print("=" * 80)
print("ğŸ“ STEP 4: Record in Tamper-Evident Ledger")
print("-" * 80)

from src.ledger.ledger import PolicyLedger, verify_chain_integrity

ledger_file = "demo_parallel_ledger.json"
ledger = PolicyLedger(ledger_file)

recorded_count = 0
for claim, result in zip(claims, verified_results):
    if result.status.value == "VALID":
        entry = ledger.append(
            policy_hash=claim.policy_hash,
            verified_reward=result.verified_reward,
            agent_id=claim.agent_id
        )
        recorded_count += 1
        print(f"âœ… {claim.agent_id:15s} â†’ Recorded (block #{entry.index})")

print(f"\nâœ… Ledger contains {len(ledger.read_all())} entries")

# Verify chain integrity
entries = ledger.read_all()
is_intact = verify_chain_integrity(entries)
print(f"ğŸ”— Hash chain: {'âœ… INTACT' if is_intact else 'âŒ BROKEN'}")
print()

# -----------------------------------------------------------------------------
# STEP 5: Marketplace Selection
# -----------------------------------------------------------------------------
print("=" * 80)
print("ğŸ† STEP 5: Marketplace Selects Best Policy")
print("-" * 80)

from src.marketplace.ranking import select_best_policy, PolicyMarketplace

best = select_best_policy(ledger)

if best:
    print(f"ğŸ† WINNER: {best.agent_id}")
    print(f"   Reward: {best.verified_reward:.3f}")
    print(f"   Hash: {best.policy_hash[:16]}...")
    print()
    
    # Show full rankings
    marketplace = PolicyMarketplace(ledger)
    rankings = marketplace.get_ranked_policies()
    print("Full Rankings:")
    for i, policy in enumerate(rankings, 1):
        medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
        print(f"  {medal} {i}. {policy.agent_id:15s} â†’ {policy.verified_reward:.3f}")
else:
    print("âŒ No valid policies found")
    exit(1)
print()

# -----------------------------------------------------------------------------
# STEP 6: Policy Reuse - THE WOW MOMENT
# -----------------------------------------------------------------------------
print("=" * 80)
print("ğŸ¯ STEP 6: Policy Reuse â€” Zero-Training Defense Deployment")
print("-" * 80)

from src.consumer.reuse import reuse_best_policy

# Get the training seed from the best policy's env_id
# Format: "cyber_defense_env_seed_{seed}_horizon_{time_horizon}"
best_agent_claim = next(c for c in claims if c.agent_id == best.agent_id)
env_parts = best_agent_claim.env_id.split("_")
training_seed = int(env_parts[env_parts.index("seed") + 1])

print(f"ğŸ”„ Reusing best policy from {best.agent_id} (trained on seed={training_seed})...")
print()

# Test 1: Same seed as training (demonstrates perfect reproducibility)
print("ğŸ“Š Test 1: Same Environment (Deterministic Replay)")
print(f"   Using seed={training_seed} (same as training)")
reuse_start = time.time()
result_same = reuse_best_policy(best, seed=training_seed, episodes=100)
reuse_time_same = time.time() - reuse_start

print(f"   âœ… Reused Score: {result_same['policy_reward']:.3f}")
print(f"   ğŸ“‹ Verified Score: {result_same['verified_reward']:.3f}")
print(f"   ğŸ“‰ Baseline (random): {result_same['baseline_reward']:.3f}")
print(f"   ğŸ“ˆ Improvement: {result_same['improvement']:+.1f}%")
print(f"   â±ï¸  Time: {reuse_time_same:.2f}s")
print()

# Test 2: Different seed (demonstrates generalization limits)
print("ğŸ“Š Test 2: Different Environment (Generalization)")
print(f"   Using seed=9999 (unseen attack patterns)")
reuse_start = time.time()
result_diff = reuse_best_policy(best, seed=9999, episodes=100)
reuse_time_diff = time.time() - reuse_start

print(f"   âœ… Reused Score: {result_diff['policy_reward']:.3f}")
print(f"   ğŸ“‰ Baseline (random): {result_diff['baseline_reward']:.3f}")
if result_diff['policy_reward'] > result_diff['baseline_reward']:
    print(f"   ğŸ“ˆ Improvement: {result_diff['improvement']:+.1f}%")
else:
    print(f"   âš ï¸  Score lower than baseline (policy trained on different patterns)")
print(f"   â±ï¸  Time: {reuse_time_diff:.2f}s (instant deployment)")
print()

print("ğŸ’¡ Key Insights:")
print("   â€¢ Same seed: Perfect reproducibility (policy matches verification)")
print("   â€¢ Different seed: Shows generalization limits (expected for tabular Q-learning)")
print("   â€¢ Both: Instant deployment without retraining!")
print()

# Use same-seed result for final summary
result = result_same
reuse_time = reuse_time_same

# -----------------------------------------------------------------------------
# FINAL SUMMARY
# -----------------------------------------------------------------------------
total_time = time.time() - overall_start

print("=" * 80)
print("ğŸ‰ CYBER DEFENSE POLICY DEMO COMPLETE!")
print("=" * 80)
print()
print(f"â±ï¸  Total Demo Time: {total_time:.1f}s")
print()
print("âœ… Complete Workflow:")
print(f"   â€¢ Training: {training_time:.1f}s ({len(claims)} agents)")
print(f"   â€¢ Verification: {verify_time:.1f}s ({valid_count} policies)")
print(f"   â€¢ Ledger: {recorded_count} entries recorded")
print(f"   â€¢ Marketplace: Best policy selected")
print(f"   â€¢ Reuse: {reuse_time:.2f}s (instant deployment!)")
print()
print("ğŸ’¡ This demonstrates:")
print("   â€¢ Decentralized learning (independent agents)")
print("   â€¢ Deterministic verification (replay guarantees)")
print("   â€¢ Tamper-evident ledger (hash-chained)")
print("   â€¢ Intelligent policy marketplace")
print("   â€¢ Zero-training policy reuse")
print(f"   â€¢ {result['improvement']:+.1f}% improvement over naive baseline")
print()
print("âš ï¸  DISCLAIMER:")
print("   This is a SIMULATED cyber defense environment for demonstrating")
print("   RL policy verification and reuse. NOT a real cybersecurity system.")
print()
print(f"ğŸ“ Ledger: {ledger_file}")
print(f"ğŸ“ Policies: policies/ directory")
print()
